{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011715,
     "end_time": "2020-11-21T03:41:36.754835",
     "exception": false,
     "start_time": "2020-11-21T03:41:36.743120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:36.786123Z",
     "iopub.status.busy": "2020-11-21T03:41:36.784270Z",
     "iopub.status.idle": "2020-11-21T03:41:38.858743Z",
     "shell.execute_reply": "2020-11-21T03:41:38.857531Z"
    },
    "papermill": {
     "duration": 2.093178,
     "end_time": "2020-11-21T03:41:38.858896",
     "exception": false,
     "start_time": "2020-11-21T03:41:36.765718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader as DL\n",
    "from torch.nn.utils import weight_norm as WN\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from time import time\n",
    "import random as r\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:38.893671Z",
     "iopub.status.busy": "2020-11-21T03:41:38.891752Z",
     "iopub.status.idle": "2020-11-21T03:41:38.894410Z",
     "shell.execute_reply": "2020-11-21T03:41:38.894919Z"
    },
    "papermill": {
     "duration": 0.024734,
     "end_time": "2020-11-21T03:41:38.895049",
     "exception": false,
     "start_time": "2020-11-21T03:41:38.870315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def breaker():\n",
    "  print(\"\\n\" + 30*\"-\" + \"\\n\")\n",
    "\n",
    "def head(x, no_of_ele=5):\n",
    "    breaker()\n",
    "    print(x[:no_of_ele])\n",
    "    breaker()\n",
    "\n",
    "def preprocess(x, *args):\n",
    "    df = x.copy()\n",
    "    df[args[0]] = df[args[0]].map({\"trt_cp\" : 0, \"ctl_vehicle\" : 1})\n",
    "    df[args[1]] = df[args[1]].map({24 : 0, 48 : 1, 72 : 2})\n",
    "    df[args[2]] = df[args[2]].map({\"D1\" : 0, \"D2\": 1})\n",
    "    return df\n",
    "\n",
    "def getCol(x):\n",
    "  return [col for col in x.columns]\n",
    "\n",
    "sc_X = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:38.927044Z",
     "iopub.status.busy": "2020-11-21T03:41:38.926276Z",
     "iopub.status.idle": "2020-11-21T03:41:45.049514Z",
     "shell.execute_reply": "2020-11-21T03:41:45.048965Z"
    },
    "papermill": {
     "duration": 6.142071,
     "end_time": "2020-11-21T03:41:45.049620",
     "exception": false,
     "start_time": "2020-11-21T03:41:38.907549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "\n",
      "TRAINING FEATURES DATASET SHAPE : (23814, 876)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "TRAINING LABELS DATASET SHAPE   : (23814, 207)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "TEST FEATURES DATASET SHAPE     : (3982, 876)\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_feat = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "tr_lbls = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "ts_feat = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "\n",
    "breaker()\n",
    "print(\"TRAINING FEATURES DATASET SHAPE :\", repr(tr_feat.shape))\n",
    "breaker()\n",
    "print(\"TRAINING LABELS DATASET SHAPE   :\", repr(tr_lbls.shape))\n",
    "breaker()\n",
    "print(\"TEST FEATURES DATASET SHAPE     :\", repr(ts_feat.shape))\n",
    "breaker()\n",
    "\n",
    "X = tr_feat.copy()\n",
    "y = tr_lbls.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:45.108494Z",
     "iopub.status.busy": "2020-11-21T03:41:45.092669Z",
     "iopub.status.idle": "2020-11-21T03:41:45.138377Z",
     "shell.execute_reply": "2020-11-21T03:41:45.138960Z"
    },
    "papermill": {
     "duration": 0.077347,
     "end_time": "2020-11-21T03:41:45.139112",
     "exception": false,
     "start_time": "2020-11-21T03:41:45.061765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_feats = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
    "              18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
    "              32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
    "              47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
    "              61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
    "              74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
    "              89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
    "              102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
    "              115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
    "              129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
    "              144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
    "              158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
    "              171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
    "              184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
    "              198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
    "              213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
    "              227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
    "              240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
    "              254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
    "              267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
    "              281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
    "              295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
    "              310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
    "              324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
    "              337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
    "              350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
    "              363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
    "              378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
    "              392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
    "              405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
    "              419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
    "              432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
    "              447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
    "              463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
    "              476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
    "              490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
    "              506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
    "              522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
    "              538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
    "              552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
    "              571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
    "              586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
    "              600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
    "              618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
    "              631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
    "              645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
    "              660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
    "              673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
    "              686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
    "              701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
    "              718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
    "              733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
    "              748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
    "              762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
    "              775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
    "              789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
    "              804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
    "              821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
    "              837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
    "              854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
    "              870, 871, 872, 873, 874]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:45.173692Z",
     "iopub.status.busy": "2020-11-21T03:41:45.172947Z",
     "iopub.status.idle": "2020-11-21T03:41:45.960224Z",
     "shell.execute_reply": "2020-11-21T03:41:45.959448Z"
    },
    "papermill": {
     "duration": 0.808867,
     "end_time": "2020-11-21T03:41:45.960384",
     "exception": false,
     "start_time": "2020-11-21T03:41:45.151517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "\n",
      "Garbage collector: collected 13 objects.\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_FEATURES = len(top_feats)\n",
    "NUM_LABELS   = tr_lbls.shape[1] - 1\n",
    "\n",
    "X = X.iloc[:, top_feats]\n",
    "y = y.drop(labels=\"sig_id\", axis=1)\n",
    "\n",
    "X = preprocess(X, \"cp_type\", \"cp_time\", \"cp_dose\")\n",
    "\n",
    "y = y.loc[X[\"cp_type\"] == 0].reset_index(drop=True)\n",
    "X = X.loc[X[\"cp_type\"] == 0].reset_index(drop=True)\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "X = sc_X.fit_transform(X)\n",
    "\n",
    "del tr_feat, tr_lbls, ts_feat, top_feats\n",
    "\n",
    "breaker()\n",
    "collected = gc.collect()\n",
    "print(\"Garbage collector: collected {} objects.\".format(collected))\n",
    "breaker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:45.994921Z",
     "iopub.status.busy": "2020-11-21T03:41:45.994166Z",
     "iopub.status.idle": "2020-11-21T03:41:45.997995Z",
     "shell.execute_reply": "2020-11-21T03:41:45.997419Z"
    },
    "papermill": {
     "duration": 0.024601,
     "end_time": "2020-11-21T03:41:45.998117",
     "exception": false,
     "start_time": "2020-11-21T03:41:45.973516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "  def __init__(this, X=None, y=None, mode=\"train\"):\n",
    "    this.mode = mode\n",
    "    this.X = X\n",
    "    if mode == \"train\":\n",
    "      this.y = y\n",
    "\n",
    "  def __len__(this):\n",
    "    return this.X.shape[0]\n",
    "\n",
    "  def __getitem__(this, idx):\n",
    "    if this.mode == \"train\":\n",
    "      return torch.FloatTensor(this.X[idx]), torch.FloatTensor(this.y[idx])\n",
    "    else:\n",
    "      return torch.FloatTensor(this.X[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:46.396174Z",
     "iopub.status.busy": "2020-11-21T03:41:46.395091Z",
     "iopub.status.idle": "2020-11-21T03:41:46.398528Z",
     "shell.execute_reply": "2020-11-21T03:41:46.397609Z"
    },
    "papermill": {
     "duration": 0.388157,
     "end_time": "2020-11-21T03:41:46.398649",
     "exception": false,
     "start_time": "2020-11-21T03:41:46.010492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG():\n",
    "  tr_batch_size = 128\n",
    "  va_batch_size = 128\n",
    "  ts_batch_size = 512\n",
    "\n",
    "  epochs  = 50\n",
    "  n_folds = 10\n",
    "  n_seeds = 5\n",
    "\n",
    "  IL = NUM_FEATURES\n",
    "  OL = NUM_LABELS\n",
    "\n",
    "  HL_1 = [2048, 1024]\n",
    "  HL_2 = [2048, 1024, 512]\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:46.434112Z",
     "iopub.status.busy": "2020-11-21T03:41:46.432195Z",
     "iopub.status.idle": "2020-11-21T03:41:46.434819Z",
     "shell.execute_reply": "2020-11-21T03:41:46.435321Z"
    },
    "papermill": {
     "duration": 0.023742,
     "end_time": "2020-11-21T03:41:46.435446",
     "exception": false,
     "start_time": "2020-11-21T03:41:46.411704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothBCELoss(nn.Module):\n",
    "    def __init__(this, eps=0.1, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "\n",
    "        this.eps = eps\n",
    "        this.reduction = reduction\n",
    "\n",
    "    def forward(this, y_pred, y_true):\n",
    "            return nn.BCELoss(reduction=this.reduction)(y_pred, y_true) * (1-this.eps) + nn.BCELoss(reduction=this.reduction)(y_pred, y_true) * (this.eps/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:46.487266Z",
     "iopub.status.busy": "2020-11-21T03:41:46.478925Z",
     "iopub.status.idle": "2020-11-21T03:41:46.489530Z",
     "shell.execute_reply": "2020-11-21T03:41:46.490001Z"
    },
    "papermill": {
     "duration": 0.0421,
     "end_time": "2020-11-21T03:41:46.490121",
     "exception": false,
     "start_time": "2020-11-21T03:41:46.448021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(this, IL=None, HL=None, OL=None):\n",
    "        super(ANN, this).__init__()\n",
    "\n",
    "        this.HL = HL\n",
    "        this.DP1 = nn.Dropout(p=0.2)\n",
    "        this.DP2 = nn.Dropout(p=0.5)\n",
    "\n",
    "        if len(HL) == 2:\n",
    "            this.BN1 = nn.BatchNorm1d(IL)\n",
    "            this.FC1 = WN(nn.Linear(IL, HL[0]))\n",
    "\n",
    "            this.BN2 = nn.BatchNorm1d(HL[0])\n",
    "            this.FC2 = WN(nn.Linear(HL[0], HL[1]))\n",
    "\n",
    "            this.BN3 = nn.BatchNorm1d(HL[1])\n",
    "            this.FC3 = WN(nn.Linear(HL[1], OL))\n",
    "\n",
    "        elif len(HL) == 3:\n",
    "            this.BN1 = nn.BatchNorm1d(IL)\n",
    "            this.FC1 = WN(nn.Linear(IL, HL[0]))\n",
    "\n",
    "            this.BN2 = nn.BatchNorm1d(HL[0])\n",
    "            this.FC2 = WN(nn.Linear(HL[0], HL[1]))\n",
    "\n",
    "            this.BN3 = nn.BatchNorm1d(HL[1])\n",
    "            this.FC3 = WN(nn.Linear(HL[1], HL[2]))\n",
    "\n",
    "            this.BN4 = nn.BatchNorm1d(HL[2])\n",
    "            this.FC4 = WN(nn.Linear(HL[2], OL))\n",
    "\n",
    "        else:\n",
    "            raise NotImplemetedError(\"Only supports Networks of Depth 2 and 3\")\n",
    "\n",
    "    def getOptimizer(this, lr=1e-3, wd=0):\n",
    "        return optim.Adam(this.parameters(), lr=lr, weight_decay=wd)\n",
    "    \n",
    "    def forward(this, x):\n",
    "        if len(this.HL) == 2:\n",
    "            x = this.BN1(x)\n",
    "            x = this.DP1(x)\n",
    "            x = F.relu(this.FC1(x))\n",
    "            x = this.BN2(x)\n",
    "            x = this.DP2(x)\n",
    "            x = F.relu(this.FC2(x))\n",
    "            x = this.BN3(x)\n",
    "            x = this.DP2(x)\n",
    "            x = torch.sigmoid(this.FC3(x))\n",
    "            return x\n",
    "        else:\n",
    "            x = this.BN1(x)\n",
    "            x = this.DP1(x)\n",
    "            x = F.relu(this.FC1(x))\n",
    "            x = this.BN2(x)\n",
    "            x = this.DP2(x)\n",
    "            x = F.relu(this.FC2(x))\n",
    "            x = this.BN3(x)\n",
    "            x = this.DP2(x)\n",
    "            x = F.relu(this.FC3(x))\n",
    "            x = this.BN4(x)\n",
    "            x = this.DP2(x)\n",
    "            x = torch.sigmoid(this.FC4(x))\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:46.533735Z",
     "iopub.status.busy": "2020-11-21T03:41:46.523245Z",
     "iopub.status.idle": "2020-11-21T03:41:46.547900Z",
     "shell.execute_reply": "2020-11-21T03:41:46.547361Z"
    },
    "papermill": {
     "duration": 0.045213,
     "end_time": "2020-11-21T03:41:46.548007",
     "exception": false,
     "start_time": "2020-11-21T03:41:46.502794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(X=None, y=None, epochs=None, \n",
    "             n_folds=None, n_seeds=None,\n",
    "             IL=None, HL=None, OL=None,\n",
    "             device=None, lr=None, wd=None,\n",
    "             tr_batch_size=None, va_batch_size=None, eps=None):\n",
    "    breaker()\n",
    "    print(\"Training ...\")\n",
    "    breaker()\n",
    "\n",
    "    LP = []\n",
    "    names = []\n",
    "    bestLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n",
    "\n",
    "    r.seed(1729)\n",
    "    seeders = [r.randint(0,99) for i in range(n_seeds)]\n",
    "    start_time = time()\n",
    "\n",
    "    for seed in seeders:\n",
    "        fold = 0\n",
    "        for tr_idx, va_idx in MultilabelStratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed).split(X, y):\n",
    "            print(\"Processing Seed {seed}, Fold {fold} ...\".format(seed=seed, fold=fold+1))\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = X[tr_idx], X[va_idx], y[tr_idx], y[va_idx]\n",
    "\n",
    "            tr_data_setup = DS(X_train, y_train)\n",
    "            va_data_setup = DS(X_valid, y_valid)\n",
    "\n",
    "            dataloaders = {\"train\" : DL(tr_data_setup, batch_size=tr_batch_size, shuffle=True, generator=torch.manual_seed(0)),\n",
    "                            \"valid\" : DL(va_data_setup, batch_size=va_batch_size, shuffle=False)\n",
    "                            }\n",
    "\n",
    "            torch.manual_seed(0)\n",
    "            model = ANN(IL, HL, OL)\n",
    "            model.to(device)\n",
    "\n",
    "            optimizer = model.getOptimizer(lr=lr, wd=wd)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, eps=1e-8, verbose=True)\n",
    "\n",
    "            for e in range(epochs):\n",
    "                epochLoss = {\"train\" : 0, \"valid\" : 0}\n",
    "                for phase in [\"train\", \"valid\"]:\n",
    "                    if phase == \"train\":\n",
    "                        model.train()\n",
    "                    else:\n",
    "                        model.eval()\n",
    "                    lossPerPass = 0\n",
    "\n",
    "                    for feats, label in dataloaders[phase]:\n",
    "                        feats, label = feats.to(device), label.to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        with torch.set_grad_enabled(phase == \"train\"):\n",
    "                            output = model(feats)\n",
    "                            loss   = SmoothBCELoss(eps=eps)(output, label)\n",
    "                            if phase == \"train\":\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "                        lossPerPass = (loss.item()/label.shape[0])\n",
    "                    epochLoss[phase] = lossPerPass\n",
    "                LP.append(epochLoss)\n",
    "                scheduler.step(epochLoss[\"valid\"])\n",
    "                name = \"./Model_{ids}_Fold_{fold}_Seed_{seed}.pt\".format(ids=len(HL), fold=fold, seed=seed)\n",
    "                names.append(name)\n",
    "                torch.save(model.state_dict(), name)\n",
    "                if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n",
    "                    bestLoss = epochLoss\n",
    "            fold += 1\n",
    "\n",
    "    breaker()\n",
    "    print(\"Time Taken to Train {n} folds for {e} epochs : {:.2f} minutes\".format((time()-start_time)/60, n=n_folds, e=epochs))\n",
    "    breaker()\n",
    "    print(\"Best Loss (Train) : {}\".format(bestLoss[\"train\"] * tr_batch_size))\n",
    "    print(\"Best Loss (Valid) : {}\".format(bestLoss[\"valid\"] * va_batch_size))\n",
    "    breaker()\n",
    "    print(\"Training Completed\")\n",
    "    breaker()\n",
    "        \n",
    "    return LP, names, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012962,
     "end_time": "2020-11-21T03:41:46.574207",
     "exception": false,
     "start_time": "2020-11-21T03:41:46.561245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T03:41:46.607169Z",
     "iopub.status.busy": "2020-11-21T03:41:46.606273Z",
     "iopub.status.idle": "2020-11-21T04:40:50.943474Z",
     "shell.execute_reply": "2020-11-21T04:40:50.944421Z"
    },
    "papermill": {
     "duration": 3544.357447,
     "end_time": "2020-11-21T04:40:50.944666",
     "exception": false,
     "start_time": "2020-11-21T03:41:46.587219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "\n",
      "Training ...\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Processing Seed 83, Fold 1 ...\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 83, Fold 2 ...\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 83, Fold 3 ...\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 83, Fold 4 ...\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 83, Fold 5 ...\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 83, Fold 6 ...\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 83, Fold 7 ...\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Processing Seed 83, Fold 8 ...\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 83, Fold 9 ...\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 83, Fold 10 ...\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 5, Fold 1 ...\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 5, Fold 2 ...\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 5, Fold 3 ...\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 5, Fold 4 ...\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 5, Fold 5 ...\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 5, Fold 6 ...\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 5, Fold 7 ...\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 5, Fold 8 ...\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Processing Seed 5, Fold 9 ...\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 5, Fold 10 ...\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 52, Fold 1 ...\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 52, Fold 2 ...\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 52, Fold 3 ...\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 52, Fold 4 ...\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 52, Fold 5 ...\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Processing Seed 52, Fold 6 ...\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 52, Fold 7 ...\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Processing Seed 52, Fold 8 ...\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 52, Fold 9 ...\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 52, Fold 10 ...\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Processing Seed 64, Fold 1 ...\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Processing Seed 64, Fold 2 ...\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 64, Fold 3 ...\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 64, Fold 4 ...\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 64, Fold 5 ...\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Processing Seed 64, Fold 6 ...\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 64, Fold 7 ...\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 64, Fold 8 ...\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 64, Fold 9 ...\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 64, Fold 10 ...\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Processing Seed 58, Fold 1 ...\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 58, Fold 2 ...\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 58, Fold 3 ...\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Processing Seed 58, Fold 4 ...\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 58, Fold 5 ...\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 58, Fold 6 ...\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 58, Fold 7 ...\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 58, Fold 8 ...\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 58, Fold 9 ...\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Processing Seed 58, Fold 10 ...\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Time Taken to Train 10 folds for 50 epochs : 59.07 minutes\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Best Loss (Train) : 0.04884727408246296\n",
      "Best Loss (Valid) : 0.05172968538183915\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Training Completed\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LP, Names, Network = train_fn(X=X, y=y, epochs=cfg.epochs, n_folds=cfg.n_folds, n_seeds=cfg.n_seeds,\n",
    "                              IL=cfg.IL, HL=cfg.HL_1, OL=cfg.OL, device=cfg.device, lr=1e-3, wd=1e-5,\n",
    "                              tr_batch_size=cfg.tr_batch_size, va_batch_size=cfg.va_batch_size, eps=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:40:51.191377Z",
     "iopub.status.busy": "2020-11-21T04:40:51.188583Z",
     "iopub.status.idle": "2020-11-21T04:40:51.194796Z",
     "shell.execute_reply": "2020-11-21T04:40:51.193846Z"
    },
    "papermill": {
     "duration": 0.130513,
     "end_time": "2020-11-21T04:40:51.194985",
     "exception": false,
     "start_time": "2020-11-21T04:40:51.064472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "\n",
      "Notebook Execution Complete\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breaker()\n",
    "print(\"Notebook Execution Complete\")\n",
    "breaker()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 3559.93786,
   "end_time": "2020-11-21T04:40:52.620105",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-21T03:41:32.682245",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
