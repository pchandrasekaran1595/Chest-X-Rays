{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoA_Analysis",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7dw5Zp80SgX",
        "outputId": "d9140d93-7c95-4d7b-9616-69ae2c3ca99a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install iterative-stratification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.17.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs-vmPVuv9sZ",
        "outputId": "6e43b008-f6c1-4850-ea27-ab705ad0b061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "datapath = \"/content/gdrive/My Drive/Datasets/MoA/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21vTzNSwHiR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from time import time\n",
        "import random as r\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader as DL\n",
        "from torch.nn.utils import weight_norm as WN\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from time import time\n",
        "import random as r\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3O5XpB6wavd"
      },
      "source": [
        "def breaker():\n",
        "    print(\"\\n\" + 30*\"-\" + \"\\n\")\n",
        "\n",
        "def head(x, no_of_ele=5):\n",
        "    breaker()\n",
        "    print(x[:no_of_ele])\n",
        "    breaker()\n",
        "\n",
        "def getCol(x):\n",
        "    return [col for col in x.columns]\n",
        "\n",
        "def preprocess(x, *args):\n",
        "    df = x.copy()\n",
        "    df[args[0]] = df[args[0]].map({\"trt_cp\" : 0, \"ctl_vehicle\" : 1})\n",
        "    df[args[1]] = df[args[1]].map({24 : 0, 48 : 1, 72 : 2})\n",
        "    df[args[2]] = df[args[2]].map({\"D1\" : 0, \"D2\": 1})\n",
        "    return df\n",
        "\n",
        "def G_Scale(x=None, scale=1):\n",
        "    mu    = x.shape[1]/2\n",
        "    sigma = x.shape[1] * (2**5)\n",
        "\n",
        "    x_1 = np.subtract(x, mu)**2\n",
        "    e_p = np.multiply(-0.5/sigma, x_1)\n",
        "\n",
        "    g_f = (1/math.sqrt(math.pi * 2)) * np.exp(e_p) * scale\n",
        "    return g_f\n",
        "\n",
        "def GaussianScaler(x=None):\n",
        "    np.random.seed(1729)\n",
        "    g_samples = np.random.normal(loc=0, scale=0.01, size=(x.shape[0], x.shape[1]))\n",
        "    g_scaled = np.multiply(g_samples, x)\n",
        "    return g_scaled\n",
        "\n",
        "def log_loss_metric(y_true, y_pred, num_classes=206):\n",
        "  metric = []\n",
        "  for i in range(num_classes):\n",
        "    metric.append(log_loss(y_true[:, i], y_pred[:, i], labels=[0, 1], eps=1e-15))\n",
        "  return sum(metric)/num_classes\n",
        "\n",
        "NUM_CLASSES  = 206\n",
        "NUM_FEATURES = 785"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfH9vQEvwWN6"
      },
      "source": [
        "top_feats = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
        "              18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "              32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
        "              47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
        "              61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
        "              74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
        "              89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
        "              102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
        "              115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "              129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
        "              144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
        "              158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
        "              171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
        "              184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
        "              198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
        "              213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
        "              227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
        "              240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
        "              254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
        "              267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
        "              281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
        "              295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
        "              310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
        "              324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
        "              337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
        "              350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
        "              363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
        "              378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
        "              392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
        "              405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
        "              419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
        "              432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
        "              447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
        "              463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
        "              476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
        "              490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
        "              506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
        "              522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
        "              538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
        "              552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
        "              571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
        "              586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
        "              600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
        "              618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
        "              631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
        "              645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
        "              660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
        "              673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
        "              686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
        "              701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
        "              718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
        "              733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
        "              748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
        "              762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
        "              775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
        "              789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
        "              804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
        "              821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
        "              837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
        "              854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
        "              870, 871, 872, 873, 874]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raQrrreAx0yP"
      },
      "source": [
        "class DS(Dataset):\n",
        "  def __init__(this, X=None, y=None, mode=\"train\"):\n",
        "    this.mode = mode\n",
        "    this.X = X\n",
        "    if mode == \"train\":\n",
        "      this.y = y\n",
        "  \n",
        "  def __len__(this):\n",
        "    return this.X.shape[0]\n",
        "\n",
        "  def __getitem__(this, idx):\n",
        "    if this.mode == \"train\":\n",
        "      return torch.FloatTensor(this.X[idx]), torch.FloatTensor(this.y[idx])\n",
        "    else:\n",
        "      return torch.FloatTensor(this.X[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjVyZlbT0fVO"
      },
      "source": [
        "class CFG():\n",
        "  tr_batch_size = 512\n",
        "  va_batch_size = 512\n",
        "  ts_batch_size = 512\n",
        "\n",
        "  epochs = 50\n",
        "\n",
        "  IL = NUM_FEATURES\n",
        "  HL = [2048, 1024]\n",
        "  OL = NUM_CLASSES\n",
        "\n",
        "  n_folds = 5\n",
        "  n_seeds = 2\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cfg = CFG()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYS3ZoZW0mT4"
      },
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(this, IL=None, HL=None, OL=None):\n",
        "        super(ANN, this).__init__()\n",
        "\n",
        "        this.HL = HL\n",
        "        this.DP1 = nn.Dropout(p=0.2)\n",
        "        this.DP2 = nn.Dropout(p=0.5)\n",
        "        this.BN1 = nn.BatchNorm1d(IL)\n",
        "        this.FC1 = WN(nn.Linear(IL, HL[0]))\n",
        "\n",
        "        this.BN2 = nn.BatchNorm1d(HL[0])\n",
        "        this.FC2 = WN(nn.Linear(HL[0], HL[1]))\n",
        "\n",
        "        this.BN3 = nn.BatchNorm1d(HL[1])\n",
        "        this.FC3 = WN(nn.Linear(HL[1], OL))\n",
        "\n",
        "    def getOptimizer(this, lr=1e-3, wd=0):\n",
        "        return optim.Adam(this.parameters(), lr=lr, weight_decay=wd)\n",
        "    \n",
        "    def forward(this, x):\n",
        "        x = this.BN1(x)\n",
        "        x = this.DP1(x)\n",
        "        x = F.relu(this.FC1(x))\n",
        "        x = this.BN2(x)\n",
        "        x = this.DP2(x)\n",
        "        x = F.relu(this.FC2(x))\n",
        "        x = this.BN3(x)\n",
        "        x = this.DP2(x)\n",
        "        x = torch.sigmoid(this.FC3(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRzGANwv02PS"
      },
      "source": [
        "def train_fn(X=None, y=None, n_folds=None, n_seeds=None):\n",
        "  breaker()\n",
        "  print(\"Training ...\")\n",
        "  breaker()\n",
        "\n",
        "  LP = []\n",
        "  names = []\n",
        "  bestLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n",
        "\n",
        "  r.seed(1729)\n",
        "  seeders = [r.randint(0,99) for i in range(n_seeds)]\n",
        "  start_time = time()\n",
        "\n",
        "  for seed in seeders:\n",
        "    fold = 0\n",
        "    for tr_idx, va_idx in MultilabelStratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed).split(X, y):\n",
        "      print(\"Processing Seed {seed}, Fold {fold} ...\".format(seed=seed, fold=fold+1))\n",
        "\n",
        "      X_train, X_valid, y_train, y_valid = X[tr_idx], X[va_idx], y[tr_idx], y[va_idx]\n",
        "\n",
        "      tr_data_setup = DS(X_train, y_train)\n",
        "      va_data_setup = DS(X_valid, y_valid)\n",
        "\n",
        "      dataloaders = {\"train\" : DL(tr_data_setup, batch_size=cfg.tr_batch_size, shuffle=True, generator=torch.manual_seed(0)),\n",
        "                     \"valid\" : DL(va_data_setup, batch_size=cfg.va_batch_size, shuffle=False)\n",
        "                    }\n",
        "\n",
        "      torch.manual_seed(0)\n",
        "      model = ANN(cfg.IL, cfg.HL, cfg.OL)\n",
        "      model.to(cfg.device)\n",
        "\n",
        "      optimizer = model.getOptimizer(lr=1e-3, wd=1e-5)\n",
        "      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, eps=1e-8, verbose=True)\n",
        "\n",
        "      for e in range(cfg.epochs):\n",
        "        epochLoss = {\"train\" : 0, \"valid\" : 0}\n",
        "        for phase in [\"train\", \"valid\"]:\n",
        "          if phase == \"train\":\n",
        "            model.train()\n",
        "          else:\n",
        "            model.eval()\n",
        "          lossPerPass = 0\n",
        "\n",
        "          for feats, label in dataloaders[phase]:\n",
        "            feats, label = feats.to(cfg.device), label.to(cfg.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(phase == \"train\"):\n",
        "              output = model(feats)\n",
        "              loss   = nn.BCELoss()(output, label)\n",
        "              if phase == \"train\":\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            lossPerPass = (loss.item()/label.shape[0])\n",
        "          epochLoss[phase] = lossPerPass\n",
        "        LP.append(epochLoss)\n",
        "        scheduler.step(epochLoss[\"valid\"])\n",
        "        name = \"./Model_Fold_{fold}_Seed_{seed}.pt\".format(fold=fold, seed=seed)\n",
        "        names.append(name)\n",
        "        torch.save(model.state_dict(), name)\n",
        "        if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n",
        "          bestLoss = epochLoss\n",
        "          #name = \"Model_Fold_{fold}_Seed_{seed}.pt\".format(fold=fold, seed=seed)\n",
        "          #names.append(name)\n",
        "          #torch.save(model.state_dict(), datapath+name)\n",
        "      fold += 1\n",
        "\n",
        "  breaker()\n",
        "  print(\"Time Taken to Train {n} folds for {e} epochs : {:.2f} minutes\".format((time()-start_time)/60, n=n_folds, e=cfg.epochs))\n",
        "  breaker()\n",
        "  print(\"Best Loss :\", repr(bestLoss))\n",
        "  breaker()\n",
        "  print(\"Training Completed\")\n",
        "  breaker()\n",
        "    \n",
        "  return LP, names, model\n",
        "\n",
        "def eval_fn(model=None, names=None, dataloader=None, num_obs_test=None):\n",
        "  y_pred = np.zeros((num_obs_test, NUM_CLASSES))\n",
        "\n",
        "  for name in names:\n",
        "    Pred = torch.zeros(cfg.ts_batch_size, NUM_CLASSES).to(cfg.device)\n",
        "    model.load_state_dict(torch.load(name))\n",
        "    model.eval()\n",
        "    for feat in dataloader:\n",
        "      feat = feat.to(cfg.device)\n",
        "      with torch.no_grad():\n",
        "        Prob = model(feat)\n",
        "      Pred = torch.cat((Pred, Prob), dim=0)\n",
        "    Pred = Pred[cfg.ts_batch_size:]\n",
        "    Pred = Pred.cpu().numpy()\n",
        "    y_pred = np.add(y_pred, Pred)\n",
        "  y_pred = np.divide(y_pred, len(names))\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHwLqX8G8c0F"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKhIE0hl1Vgw",
        "outputId": "0e6444c3-6192-4f7b-dc3a-81df15637b9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "########## PREPROCESSING ##########\n",
        "tr_feat = pd.read_csv(datapath + \"train.csv\")\n",
        "tr_lbls = pd.read_csv(datapath + \"train_targets_s.csv\")\n",
        "\n",
        "tr_feat = tr_feat.iloc[:, top_feats].copy()\n",
        "tr_lbls = tr_lbls.drop(labels=\"sig_id\", axis=1)\n",
        "\n",
        "tr_feat = preprocess(tr_feat, \"cp_type\", \"cp_time\", \"cp_dose\")\n",
        "\n",
        "tr_lbls = tr_lbls.loc[tr_feat[\"cp_type\"] == 0].reset_index(drop=True)\n",
        "tr_feat = tr_feat.loc[tr_feat[\"cp_type\"] == 0].reset_index(drop=True)\n",
        "\n",
        "features = tr_feat.copy().values\n",
        "labels   = tr_lbls.copy().values\n",
        "\n",
        "X, X_test, y, y_test = train_test_split(features, labels, test_size=6948, shuffle=True, random_state=0)\n",
        "X, X_test, y, y_test = X.astype(float), X_test.astype(float), y.astype(float), y_test.astype(float)\n",
        "\n",
        "del tr_feat, tr_lbls, features, labels\n",
        "\n",
        "########## TRAINING ###########\n",
        "LP, names, Network = train_fn(X=X, y=y, n_folds=cfg.n_folds, n_seeds=cfg.n_seeds)\n",
        "\n",
        "########## EVALUATION #########\n",
        "ts_data_setup = DS(X_test, None, \"test\")\n",
        "ts_data = DL(ts_data_setup, batch_size=cfg.ts_batch_size, shuffle=False)\n",
        "\n",
        "y_pred = eval_fn(Network, set(names), ts_data, ts_data_setup.__len__())\n",
        "\n",
        "print(log_loss_metric(y_test, y_pred))\n",
        "breaker()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------\n",
            "\n",
            "Training ...\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Processing Seed 83, Fold 1 ...\n",
            "Processing Seed 83, Fold 2 ...\n",
            "Processing Seed 83, Fold 3 ...\n",
            "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Processing Seed 83, Fold 4 ...\n",
            "Processing Seed 83, Fold 5 ...\n",
            "Processing Seed 5, Fold 1 ...\n",
            "Processing Seed 5, Fold 2 ...\n",
            "Processing Seed 5, Fold 3 ...\n",
            "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Processing Seed 5, Fold 4 ...\n",
            "Processing Seed 5, Fold 5 ...\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Time Taken to Train 5 folds for 50 epochs : 8.71 minutes\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Best Loss : {'train': 7.380161800288729e-05, 'valid': 3.811570985073393e-05}\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Training Completed\n",
            "\n",
            "------------------------------\n",
            "\n",
            "0.01749659405471929\n",
            "\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VDXko8q8JU6"
      },
      "source": [
        "# Min Max Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cqlQWF87-kd",
        "outputId": "004259f0-855a-4a76-bee4-26c2abc1db41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tr_feat = pd.read_csv(datapath + \"train.csv\")\n",
        "tr_lbls = pd.read_csv(datapath + \"train_targets_s.csv\")\n",
        "\n",
        "tr_lbls = tr_lbls.drop(labels=\"sig_id\", axis=1)\n",
        "\n",
        "tr_feat = preprocess(tr_feat, \"cp_type\", \"cp_time\", \"cp_dose\")\n",
        "\n",
        "tr_lbls = tr_lbls.loc[tr_feat[\"cp_type\"] == 0].reset_index(drop=True)\n",
        "tr_feat = tr_feat.loc[tr_feat[\"cp_type\"] == 0].reset_index(drop=True)\n",
        "\n",
        "features = tr_feat.copy().values\n",
        "labels   = tr_lbls.copy().values\n",
        "\n",
        "X, X_test, y, y_test = train_test_split(features, labels, test_size=6948, shuffle=True, random_state=0)\n",
        "\n",
        "X, X_test, y, y_test = X[:, top_feats].astype(float), X_test[:, top_feats].astype(float), y.astype(float), y_test.astype(float)\n",
        "\n",
        "mms  = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
        "X = mms.fit_transform(X)\n",
        "X_test = mms.transform(X_test)\n",
        "\n",
        "del tr_feat, tr_lbls, features, labels\n",
        "\n",
        "########## TRAINING ###########\n",
        "LP, names, Network = train_fn(X=X, y=y, n_folds=cfg.n_folds, n_seeds=cfg.n_seeds)\n",
        "\n",
        "########## EVALUATION #########\n",
        "ts_data_setup = DS(X_test, None, \"test\")\n",
        "ts_data = DL(ts_data_setup, batch_size=cfg.ts_batch_size, shuffle=False)\n",
        "\n",
        "y_pred = eval_fn(Network, set(names), ts_data, ts_data_setup.__len__())\n",
        "\n",
        "print(log_loss_metric(y_test, y_pred))\n",
        "breaker()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------\n",
            "\n",
            "Training ...\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Processing Seed 83, Fold 1 ...\n",
            "Processing Seed 83, Fold 2 ...\n",
            "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Processing Seed 83, Fold 3 ...\n",
            "Processing Seed 83, Fold 4 ...\n",
            "Processing Seed 83, Fold 5 ...\n",
            "Processing Seed 5, Fold 1 ...\n",
            "Processing Seed 5, Fold 2 ...\n",
            "Processing Seed 5, Fold 3 ...\n",
            "Processing Seed 5, Fold 4 ...\n",
            "Processing Seed 5, Fold 5 ...\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Time Taken to Train 5 folds for 50 epochs : 5.14 minutes\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Best Loss : {'train': 7.164323635931526e-05, 'valid': 3.8144860247319396e-05}\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Training Completed\n",
            "\n",
            "------------------------------\n",
            "\n",
            "0.017563298349693765\n",
            "\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMtRM0F8DOSZ"
      },
      "source": [
        "# Standard Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irCPhQq7ADsI",
        "outputId": "7a357dbf-199a-4507-c448-9caa85ccb9ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tr_feat = pd.read_csv(datapath + \"train.csv\")\n",
        "tr_lbls = pd.read_csv(datapath + \"train_targets_s.csv\")\n",
        "\n",
        "tr_lbls = tr_lbls.drop(labels=\"sig_id\", axis=1)\n",
        "\n",
        "tr_feat = preprocess(tr_feat, \"cp_type\", \"cp_time\", \"cp_dose\")\n",
        "\n",
        "tr_lbls = tr_lbls.loc[tr_feat[\"cp_type\"] == 0].reset_index(drop=True)\n",
        "tr_feat = tr_feat.loc[tr_feat[\"cp_type\"] == 0].reset_index(drop=True)\n",
        "\n",
        "features = tr_feat.copy().values\n",
        "labels   = tr_lbls.copy().values\n",
        "\n",
        "X, X_test, y, y_test = train_test_split(features, labels, test_size=6948, shuffle=True, random_state=0)\n",
        "\n",
        "X, X_test, y, y_test = X[:, top_feats].astype(float), X_test[:, top_feats].astype(float), y.astype(float), y_test.astype(float)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "X = sc_X.fit_transform(X)\n",
        "X_test = sc_X.transform(X_test)\n",
        "\n",
        "del tr_feat, tr_lbls, features, labels\n",
        "\n",
        "########## TRAINING ###########\n",
        "LP, names, Network = train_fn(X=X, y=y, n_folds=cfg.n_folds, n_seeds=cfg.n_seeds)\n",
        "\n",
        "########## EVALUATION #########\n",
        "ts_data_setup = DS(X_test, None, \"test\")\n",
        "ts_data = DL(ts_data_setup, batch_size=cfg.ts_batch_size, shuffle=False)\n",
        "\n",
        "y_pred = eval_fn(Network, set(names), ts_data, ts_data_setup.__len__())\n",
        "\n",
        "print(log_loss_metric(y_test, y_pred))\n",
        "breaker()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------\n",
            "\n",
            "Training ...\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Processing Seed 83, Fold 1 ...\n",
            "Processing Seed 83, Fold 2 ...\n",
            "Processing Seed 83, Fold 3 ...\n",
            "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Processing Seed 83, Fold 4 ...\n",
            "Processing Seed 83, Fold 5 ...\n",
            "Processing Seed 5, Fold 1 ...\n",
            "Processing Seed 5, Fold 2 ...\n",
            "Processing Seed 5, Fold 3 ...\n",
            "Processing Seed 5, Fold 4 ...\n",
            "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Processing Seed 5, Fold 5 ...\n",
            "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Time Taken to Train 5 folds for 50 epochs : 8.61 minutes\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Best Loss : {'train': 7.159356859379582e-05, 'valid': 3.8143746893514285e-05}\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Training Completed\n",
            "\n",
            "------------------------------\n",
            "\n",
            "0.017525802695967343\n",
            "\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5Oj2zE92gUL"
      },
      "source": [
        "# Gaussian Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlHDh1eh2ytD",
        "outputId": "ab6f6fe8-d991-4c96-8b60-f92783872285",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tr_feat = pd.read_csv(datapath + \"train.csv\")\n",
        "tr_lbls = pd.read_csv(datapath + \"train_targets_s.csv\")\n",
        "\n",
        "tr_lbls = tr_lbls.drop(labels=\"sig_id\", axis=1)\n",
        "\n",
        "tr_feat = preprocess(tr_feat, \"cp_type\", \"cp_time\", \"cp_dose\")\n",
        "\n",
        "tr_lbls = tr_lbls.loc[tr_feat[\"cp_type\"] == 0].reset_index(drop=True)\n",
        "tr_feat = tr_feat.loc[tr_feat[\"cp_type\"] == 0].reset_index(drop=True)\n",
        "\n",
        "features = tr_feat.copy().values\n",
        "labels   = tr_lbls.copy().values\n",
        "\n",
        "X, X_test, y, y_test = train_test_split(features, labels, test_size=6948, shuffle=True, random_state=0)\n",
        "\n",
        "X, X_test, y, y_test = X[:, top_feats].astype(float), X_test[:, top_feats].astype(float), y.astype(float), y_test.astype(float)\n",
        "\n",
        "X = G_Scale(X)\n",
        "X_test = G_Scale(X_test)\n",
        "\n",
        "del tr_feat, tr_lbls, features, labels\n",
        "\n",
        "########## TRAINING ###########\n",
        "LP, names, Network = train_fn(X=X, y=y, n_folds=cfg.n_folds, n_seeds=cfg.n_seeds)\n",
        "\n",
        "########## EVALUATION #########\n",
        "ts_data_setup = DS(X_test, None, \"test\")\n",
        "ts_data = DL(ts_data_setup, batch_size=cfg.ts_batch_size, shuffle=False)\n",
        "\n",
        "y_pred = eval_fn(Network, set(names), ts_data, ts_data_setup.__len__())\n",
        "\n",
        "print(log_loss_metric(y_test, y_pred))\n",
        "breaker()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15000, 785)\n",
            "(15000, 785)\n",
            "(6948, 785)\n",
            "(6948, 785)\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Training ...\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Processing Seed 83, Fold 1 ...\n",
            "Processing Seed 83, Fold 2 ...\n",
            "Processing Seed 83, Fold 3 ...\n",
            "Processing Seed 83, Fold 4 ...\n",
            "Processing Seed 83, Fold 5 ...\n",
            "Processing Seed 5, Fold 1 ...\n",
            "Processing Seed 5, Fold 2 ...\n",
            "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Processing Seed 5, Fold 3 ...\n",
            "Processing Seed 5, Fold 4 ...\n",
            "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Processing Seed 5, Fold 5 ...\n",
            "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Time Taken to Train 5 folds for 50 epochs : 5.10 minutes\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Best Loss : {'train': 7.452434926692928e-05, 'valid': 3.7801257249983874e-05}\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Training Completed\n",
            "\n",
            "------------------------------\n",
            "\n",
            "0.017443543977622728\n",
            "\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEX_-Nzu-yZK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}